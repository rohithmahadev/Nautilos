{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5ce67044-a790-4cce-a06f-94546ac9a4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from groq import Groq\n",
    "from langchain_groq import ChatGroq\n",
    "import chromadb\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_community.document_loaders import PyPDFLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b23881d8-11d3-45f3-8883-aa77f108b059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your Groq API key:  ········\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if \"GROQ_API_KEY\" not in os.environ:\n",
    "    os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"Enter your Groq API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8031e47-e8f1-4d83-a0e0-7ec3f94309fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "llm = ChatGroq(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    temperature=0,\n",
    "   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d66bfe4-8e55-4767-9247-8b4bce65ad22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Artificial Intelligence (AI) refers to the development of computer systems that can perform tasks that typically require human intelligence, such as learning, problem-solving, decision-making, and perception. AI involves the creation of algorithms and models that enable machines to process and analyze vast amounts of data, allowing them to make predictions, classify objects, and take actions autonomously.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = llm.invoke(\"In 2 sentences explain what is AI?\")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "25151819-56ea-43ab-b329-71c47cbf0d90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\n\\n\\n\\n\\n\\n\\nSenior Data Engineer Job at Scotiabank | CareerBeacon\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\t\\t\\t\\tSearch Jobs\\t\\t\\t\\n\\n\\n\\n\\t\\t\\t\\tSearch Employers\\t\\t\\t\\n\\n\\n\\n\\n\\t\\t\\t\\tSalary Tools\\n\\n\\n\\n\\nSalary Guide\\n\\n\\n\\n\\nIncome Tax Calculator\\n\\n\\n\\n\\nSalary Converter\\n\\n\\n\\n\\nMinimum Hourly Wage\\n\\n\\n\\n\\nCost of Living Calculator\\n\\n\\n\\n\\nExplore Demographics by Place\\n\\n\\n\\n\\n\\n\\n\\n\\t\\t\\t\\tResources\\n\\n\\n\\n\\nFor students and new grads\\n\\n\\n\\n\\nBlog\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nFR\\n\\n\\n\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tPost a Job\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\n\\nLog\\xa0In\\t\\t\\t\\t\\n\\n\\n\\n\\t\\t\\t\\t\\tSign Up\\t\\t\\t\\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSearch Jobs\\n\\n\\n\\n\\n\\n\\nSearch Employers\\n\\n\\n\\n\\n\\n\\nSalary Tools\\n\\n\\n\\n\\n\\n\\nSalary Guide\\n\\n\\n\\n\\nIncome Tax Calculator\\n\\n\\n\\n\\nSalary Converter\\n\\n\\n\\n\\nMinimum Hourly Wage\\n\\n\\n\\n\\nCost of Living Calculator\\n\\n\\n\\n\\nExplore Demographics by Place\\n\\n\\n\\n\\n\\n\\n\\nResources\\n\\n\\n\\n\\n\\n\\nFor students and new grads\\n\\n\\n\\n\\nBlog\\n\\n\\n\\n\\n\\n\\n\\nLog\\xa0In\\n\\n\\n\\n\\n\\n\\nSign Up\\n\\n\\n\\n\\n\\n\\t\\t\\t\\t\\tFor Employers\\t\\t\\t\\t\\n\\n\\n\\n\\nFranÃ§ais\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nJob Title or Location\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nJob Title, Keywords, or Employer\\n\\n\\n\\nRECENT SEARCHES\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCity, Province/Territory or \"remote\"\\n\\n\\n\\n\\n\\nFind Jobs\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\t\\t\\t\\tSenior Data Engineer\\t\\t\\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nShare Job\\n\\n\\n\\n\\nCopy Link\\n\\n\\n\\n\\n\\nFacebook\\n\\n\\n\\n\\n\\nLinkedIn\\n\\n\\n\\n\\n\\nX\\n\\n\\n\\n\\n\\nEmail\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\t\\t\\t\\t\\t\\tScotiabank\\t\\t\\t\\t\\t\\n-\\n\\n\\t\\t\\t\\t\\t\\t1,321 Jobs\\t\\t\\t\\t\\t\\n\\n\\nToronto, ON \\n\\n\\n\\n\\n\\n\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tApply Now\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\n\\n\\n\\n\\n\\nAm I a good fit?\\t\\t\\t\\t\\t\\t\\t\\n\\n\\n\\nView my job fit\\t\\t\\t\\t\\t\\t\\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tPosted 2 days ago\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\n\\n\\n\\n\\n\\nAm I a good fit? \\n\\t\\t\\t\\t\\t\\t\\n\\n\\n\\nView my job fit\\t\\t\\t\\t\\t\\t\\n\\n\\n\\n\\n \\n\\n\\n\\t\\t\\t\\t\\t\\t\\t\\tJob Details:\\t\\t\\t\\t\\t\\t\\t\\n\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tIn-person\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tFull-time\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tExperienced\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\n\\n\\n\\nRequisition ID: 244508Join a purpose driven winning team, committed to results, in an inclusive and high-performing culture. The Wealth Data engineering team within the Global Wealth Engineering (GWE) is the key team in meeting the operational data needs of the various stake holders within Wealth Management. The Lead Data Engineer will play a key role in designing and implementing data solutions using Big Data, Hadoop (Cloudera) and Google cloud working closely with the enterprise data team and data architects, solution architects, business systems analyst and data engineersIs this role right for you? In this role you will:Leading development efforts in ingesting and transforming data from various sources. Working in the Big Data/ Hadoop environment, should be hands on in writing code, building scripts, writing specifications and responsible for end to end delivery of data in the Enterprise Data Lake environmentBuild distributed, reliable and scalable data pipelines to ingest and process data from multiple data sourcesDesigning, building, operationalizing the data platform using Google Cloud Platform (GCP) data services such as DataProc, Dataflow, CloudSQL, BigQuery, CloudSpanner in combination with third parties such as Spark, Apache Beam/ composer, DBT, Cloud PubSub, Confluent Kafka, Cloud storage Cloud Functions & GithubDesigning and implementing data ingestion patterns that will support batch, streaming and API interface on both the Ingress and Egress.Guide a team of data engineers and work hands on in developing framework and custom code using best practices that will meet the demanding performance requirementsTake a lead in designing and building production data pipelines from data ingestion to consumption using GCP services, Java, Python, Scala, BigQuery, DBT, SQL etc.Experience using Cloud Dataflow using Java/Python for deploying streaming jobs in GCP as well as batch jobs using text/JSON files and writing them to BigQueryBuilding and managing data pipelines with a deep understanding of workflow orchestration, task scheduling and dependency managementProvide end-to-end technical guidance and expertise on how to effectively use Google Cloud to build solutions; creatively applying cloud infrastructure and platform services to help solve business problems; and communicating these approaches to different business usersProvide guidance on Implementing application logging, notification, jobs monitoring and performance monitoringDo you have the skills that will enable you to succeed in this role? We\\'d love to work with you if you have:10+ years of experience in data engineering, performance optimization for large OLTP applicationsExpertise knowledge of Hadoop HDFS, Hive, Pig, Flume and Sqoop.Experience with the primary managed data services within GCP, including DataProc, Dataflow, BigQuery/DBT, Cloud Spanner, Cloud SQL, Cloud Pub/Sub etc.Experience with Google Cloud Platform Databases (SQL, Spanner, PostgreSQL)Working experience in HQLGood knowledge of the concepts of Hadoop.Experience working with relational/NoSQL databasesExperience with data streaming and technologies such as Kafka, Spark-streaming etc.Experience with Infrastructure as Code (IaC) practices and frameworks like TerraformKnowledge of Java microservices and Spring BootStrong architecture knowledge with experience in providing technical solutions for cloud infrastructure.Working knowledge of developing and scaling JAVA REST services, using frameworks such as SpringGood communication and problem-solving skills. Ability to effectively convey ideas to business and technical teamsNice-To-Have Skills:Understanding of Wealth business line and the various data domains required for building an end to end solutionWhat\\'s in it for you?Diversity, Equity, Inclusion & Allyship - We strive to create an inclusive culture where every employee is empowered to reach their fullest potential, respected for who they are, and are embraced through bias-free practices and inclusive values across Scotiabank. We embrace diversity and provide opportunities for all employee to learn, grow & participate through our various Employee Resource Groups (ERGs) that span across diverse gender identities, ethnicity, race, age, ability & veterans.Accessibility and Workplace Accommodations - We value the unique skills and experiences each individual brings to the Bank, and are committed to creating and maintaining an inclusive and accessible environment for everyone. Scotiabank continues to locate, remove and prevent barriers so that we can build a diverse and inclusive environment while meeting accessibility requirements. Upskilling through online courses, cross-functional development opportunities, and tuition assistance. Competitive Rewards program including bonus, flexible vacation, personal, sick days and benefits will start on day one.Dynamic Ecosystem - Free tea & coffee, universal washrooms, and lots of space for team collaboration. Location(s): Canada : Ontario : Toronto Scotiabank is a leading bank in the Americas. Guided by our purpose: \"for every future\", we help our customers, their families and their communities achieve success through a broad range of advice, products and services, including personal and commercial banking, wealth management and private banking, corporate and investment banking, and capital markets. At Scotiabank, we value the unique skills and experiences each individual brings to the Bank, and are committed to creating and maintaining an inclusive and accessible environment for everyone. If you require accommodation (including, but not limited to, an accessible interview site, alternate format documents, ASL Interpreter, or Assistive Technology) during the recruitment and selection process, please let our Recruitment team know. If you require technical assistance, please click here. Candidates must apply directly online to be considered for this role. We thank all applicants for their interest in a career at Scotiabank; however, only those candidates who are selected for an interview will be contacted. \\n\\n\\n\\n\\nCompetition Number: \\n244508\\n\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t#Accounting and Financial jobs\\n\\n\\n\\n\\n\\n\\n\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tApply Now\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\n\\n\\n\\nSave\\n\\n\\n\\n\\n\\nShare This Job:\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tGet matched to similar opportunities like this one.\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tCreate My Account\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\n\\n\\n\\n\\n\\n\\nMore Jobs at Scotiabank\\n\\nFinancial Advisor - Place D\\'Orleans, ON\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tOttawa, ON\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\n\\nManager, Global HR Risk Management\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tToronto, ON\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\n\\nSenior Manager, Credit Cards Performance & Insights (Contract)\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tToronto, ON\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\nAll jobs from this employer\\n\\n\\n\\n\\n\\nSimilar Jobs\\n\\nAssistant Manager, Middle Office, GWO (6-Month Contract)\\nScotiabank\\nToronto, ON\\n\\n\\n\\nDirector, Financial Reporting\\nScotiabank\\nToronto, ON\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nFor Jobseekers\\n\\n\\n\\n\\nGet Hired\\nSearch Jobs\\nStudents & Recent Grads\\nBrowse Jobs\\nSearch Employers\\nExplore Demographics by Place\\nIncome Tax Calculator\\nSalary Converter\\nCost of Living Calculator\\nIncome Tax Calculator\\nMinimum Hourly Wage 2026\\nCareer Advice\\n\\n\\n\\n\\n\\nFor Employers\\n\\n\\n\\n\\nPost a Job\\nApplicant Tracking System\\nRecruitment Marketing Services\\nJob Description Generator\\nJob Post Visualizer\\n\\nContact Sales\\n\\n\\n\\n\\n\\nAbout Us\\n\\n\\n\\n\\nWho We Are\\nBlog\\nContact Us\\nTerms of Service\\nPrivacy Policy\\nLatest Updates\\nAnnouncement\\n\\n\\n\\n\\n\\n\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tEnglish\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\n|\\n\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tFrançais\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\t\\t\\t\\t\\t© 2026  CareerBeacon.  All rights reserved.\\t\\t\\t\\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nEnglish\\nFrançais\\n\\n\\n\\n\\n\\nFor Jobseekers\\n\\nGet Hired\\nSearch Jobs\\nStudents & Recent Grads\\nBrowse Jobs\\nSearch Employers\\nExplore Demographics by Place\\nIncome Tax Calculator\\nSalary Converter\\nCost of Living Calculator\\nSalary Guide\\nMinimum Hourly Wage 2026\\nCareer Advice\\n\\n\\n\\nFor Employers\\n\\nPost a Job\\nApplicant Tracking System\\nRecruitment Marketing Services\\nJob Description Generator\\nJob Post Visualizer\\n\\nContact Sales\\n\\n\\n\\nAbout Us\\n\\nWho We Are\\nBlog\\nContact Us\\nTerms of Service\\nPrivacy Policy\\nLatest Updates\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\t\\t\\t\\t\\t© 2026  CareerBeacon.  All rights reserved.\\t\\t\\t\\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\t\\t\\t\\t\\tThis AI feature is in beta and may not always be accurate. Please verify important details.\\t\\t\\t\\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\t\\t\\t\\t\\t\\t\\t\\tApply Now \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nJob Description\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\t\\t\\t\\t\\t\\t\\t\\tHang tight, we\\'re decoding your career DNA.\\t\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\t\\t\\tWe\\'re comparing your skills and experience with this job. You\\'ll see how you fit soon! ðŸ§©\\t\\t\\t\\t\\t\\t\\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\t\\t\\t\\t\\t\\tWant to see how you fit?\\t\\t\\t\\t    \\n\\n\\t\\t\\t\\t\\t\\tUpload your resume to get AI-powered insights on your job match.\\t\\t\\t\\t\\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\t\\t\\t\\t\\t\\tResume Upload Error\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\tFor some reason we couldnâ€™t process your resume. Double check the file format and size and try again.  Weâ€™re sorry!\\t\\t\\t\\t\\t\\n\\n\\n\\n\\nUpload Resume\\n\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t(Max file size: 20MB â€“ .pdf, .docx, .doc, .rtf, .txt)\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\n\\t\\t\\t\\t\\t\\t\\t\\tYour resume is private and used only for job matching.\\t\\t\\t\\t\\t\\t\\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\t\\t\\t\\t\\t\\t\"Am I a Good Fit?\" is for jobseekers only\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\tThis feature is not available in employer accounts.\\t\\t\\t\\t\\t\\n\\n\\n\\n\\n\\t\\t\\t\\t\\tGo to Employer Dashboard\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\tClose\\t\\t\\t\\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\t\\t\\t\\t\\t\\'Am I a good fit?\\' Guidance âœ¨\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\tUpload your resume to see if you\\'re a good fit for any job with our AI-powered fit indicator. Get the help that highlights your strengths!\\t\\t\\t\\t\\n\\n\\n\\n\\n\\n\\t\\t\\t\\t\\t\\t\\tCreate a Free Account\\t\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\t\\tI already have one!\\t\\t\\t\\t\\t\\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\t\\t\\t\\t\\tLog\\xa0In\\t\\t\\t\\t\\n\\nSave now, apply later.\\nGet better matches and find your perfect fit\\n\\n\\n\\n\\n\\n\\n\\n\\nEmail *\\n\\n\\n\\nPassword *\\n\\n\\nForgot Password?\\n\\n\\n\\t\\t\\t\\t\\t\\t\\t\\tContinue\\t\\t\\t\\t\\t\\t\\t\\n\\n\\n\\n\\n\\nor sign in with\\n\\n\\n\\n\\nGoogle\\n\\n\\n\\nLinkedIn\\n\\n\\n\\nFacebook\\n\\n\\n\\n\\n\\n \\nDon\\'t have an account?\\nCreate An Account\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nBefore you go...\\n\\nLet us keep searching and send similar opportunities to your inbox.\\n\\n\\n\\n\\n\\n\\n\\nGoogle\\n\\n\\n\\nLinkedIn\\n\\n\\n\\nFacebook\\n\\n\\n\\n\\n\\n\\n\\n\\nor sign up with\\n\\n\\n\\n\\n\\n\\n\\n\\nEmail *\\n\\n\\n\\nPassword *\\n\\nMinimum of 8 characters with at least 1 upper and lower case letter and 1 number.\\n\\n\\n\\t\\t\\t\\t\\t\\t\\t\\tCreate My Account\\t\\t\\t\\t\\t\\t\\t\\n\\n\\n\\n\\n\\n\\n\\n\\nAlready have an account?\\nLog\\xa0In\\n\\n\\n\\n\\n\\n\\n\\t\\t\\t\\t\\tSkip & Continue to Application >\\t\\t\\t\\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = WebBaseLoader(\"https://www.careerbeacon.com/en/job/2195743/scotiabank/senior-data-engineer/toronto\")\n",
    "loader.requests_per_second = 1\n",
    "page_data = loader.load().pop().page_content\n",
    "page_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1e60b511-d63f-4f5a-9bc1-e3a7f7ab26fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_extract = PromptTemplate.from_template(\n",
    " \"\"\"\n",
    " ##Job Description\n",
    " {page_data}\n",
    " ### Instruction:\n",
    " The scrapted text is from the career's page of a company's website. The instruction for you is to extract the job postings and return them in JSON format containing the \n",
    " follwing keys:\n",
    "\n",
    "Role:\n",
    "Experience:\n",
    "Skills (Give 5-10 skills in a list format):\n",
    "Description:\n",
    "Salary: (only if mentioned):\n",
    "\n",
    "Only return the valid JSON file.\n",
    "\n",
    "### Valid JSON Format without any preamble\n",
    " \n",
    " \"\"\" \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4b6109bf-0350-4f44-bb58-3cd6f934b71b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"Role\": \"Senior Data Engineer\",\n",
      "  \"Experience\": \"10+ years of experience in data engineering, performance optimization for large OLTP applications\",\n",
      "  \"Skills\": [\n",
      "    \"Hadoop HDFS\",\n",
      "    \"Hive\",\n",
      "    \"Pig\",\n",
      "    \"Flume\",\n",
      "    \"Sqoop\",\n",
      "    \"Google Cloud Platform Databases (SQL, Spanner, PostgreSQL)\",\n",
      "    \"HQL\",\n",
      "    \"Kafka\",\n",
      "    \"Spark-streaming\",\n",
      "    \"Infrastructure as Code (IaC) practices and frameworks like Terraform\",\n",
      "    \"Java microservices and Spring Boot\"\n",
      "  ],\n",
      "  \"Description\": \"Leading development efforts in ingesting and transforming data from various sources. Working in the Big Data/ Hadoop environment, should be hands on in writing code, building scripts, writing specifications and responsible for end to end delivery of data in the Enterprise Data Lake environment\",\n",
      "  \"Salary\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "chain_extract = prompt_extract | llm\n",
    "res = chain_extract.invoke(input = {'page_data': page_data})\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b671db05-284b-4859-a73f-baf2a9a6f7e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Role': 'Senior Data Engineer',\n",
       " 'Experience': '10+ years of experience in data engineering, performance optimization for large OLTP applications',\n",
       " 'Skills': ['Hadoop HDFS',\n",
       "  'Hive',\n",
       "  'Pig',\n",
       "  'Flume',\n",
       "  'Sqoop',\n",
       "  'Google Cloud Platform Databases (SQL, Spanner, PostgreSQL)',\n",
       "  'HQL',\n",
       "  'Kafka',\n",
       "  'Spark-streaming',\n",
       "  'Infrastructure as Code (IaC) practices and frameworks like Terraform',\n",
       "  'Java microservices and Spring Boot'],\n",
       " 'Description': 'Leading development efforts in ingesting and transforming data from various sources. Working in the Big Data/ Hadoop environment, should be hands on in writing code, building scripts, writing specifications and responsible for end to end delivery of data in the Enterprise Data Lake environment',\n",
       " 'Salary': None}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "json_parser = JsonOutputParser()\n",
    "json_parsed_job_data = json_parser.parse(res.content)\n",
    "json_parsed_job_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87a0637-70f9-44b8-8977-ae67f69a92fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "04583f39-d8ea-48af-8d95-14abc153446f",
   "metadata": {},
   "source": [
    "## Extracting releavant skills from the PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "92f3f1f6-0815-4435-812c-00db3acdbb50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.pdf.PyPDFLoader at 0x1d97c81eb40>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "file_path = \"sample1.pdf\"\n",
    "loader = PyPDFLoader(file_path)\n",
    "loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "53b74c9f-e054-4f5b-8268-a08731646a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "incorrect startxref pointer(3)\n",
      "parsing for Object Streams\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sarah Chen\n",
      " \n",
      "Data Engineer | san.francisco@email.com | (415) 555\n",
      "-\n",
      "0123 | linkedin.com/in/sarachen | github.com/sarachen\n",
      " \n",
      "PROFESSIONAL SUMMARY\n",
      " \n",
      "Results\n",
      "-\n",
      "driven Data Engineer with 8+ years of experience designing and implementing scalable data pipelines, \n",
      "warehousing solutions, and ETL processes. Expertise in building robust data infrastructure across cloud \n",
      "platforms (AWS, Azure, GCP) supporting anal\n",
      "ytics and ML applications. Proven track record of optimizing data \n",
      "systems that process petabytes of data daily.\n",
      " \n",
      "TECHNICAL SKILLS\n",
      " \n",
      "Languages: \n",
      "Python, SQL, Scala, Java, Bash\n",
      " \n",
      "Data Processing: \n",
      "Apache Spark, Kafka, Airflow, Databricks, dbt, Flink\n",
      " \n",
      "Cloud Platforms: \n",
      "AWS (Redshift, EMR, Glue, S3, Lambda), Azure (Synapse, Data Factory), GCP (BigQuery, \n",
      "Dataflow)\n",
      " \n",
      "Databases: \n",
      "PostgreSQL, MySQL, MongoDB, Cassandra, Redis, Snowflake\n",
      " \n",
      "Tools & Frameworks: \n",
      "Docker, Kubernetes, Terraform, Git, Jenkins, Tableau, Looker\n",
      " \n",
      "PROFESSIONAL EXPERIENCE\n",
      " \n",
      "Senior Data Engineer\n",
      " \n",
      "| TechCorp Inc. | San Francisco, CA | June 2020 \n",
      "–\n",
      " \n",
      "Present\n",
      " \n",
      "•\n",
      " \n",
      "Architected and deployed real\n",
      "-\n",
      "time streaming data pipelines using Kafka and Spark Streaming, \n",
      "processing 5TB+ daily across 200+ microservices\n",
      " \n",
      "•\n",
      " \n",
      "Reduced data warehouse query latency by 65% through optimization of Snowflake schemas and \n",
      "implementation of incremental materialized views\n",
      " \n",
      "•\n",
      " \n",
      "Led migration of legacy ETL processes to modern ELT framework using dbt and Airflow, improving data \n",
      "freshness from 24hrs to 2hrs\n",
      " \n",
      "•\n",
      " \n",
      "Built automated data quality monitoring framework using Great Expectations, reducing data incidents \n",
      "by 80%\n",
      " \n",
      "•\n",
      " \n",
      "Mentored team of 4 junior engineers and established best practices for data pipeline development and \n",
      "documentation\n",
      " \n",
      "Data Engineer\n",
      " \n",
      "| DataFlow Solutions | Seattle, WA | March 2018 \n",
      "–\n",
      " \n",
      "May 2020\n",
      " \n",
      "•\n",
      " \n",
      "Designed and implemented AWS\n",
      "-\n",
      "based data lake architecture handling 2PB of structured and \n",
      "unstructured data using S3, Glue, and Athena\n",
      " \n",
      "•\n",
      " \n",
      "Developed 50+ production ETL pipelines using Python and Apache Airflow, ensuring 99.9% SLA \n",
      "compliance\n",
      " \n",
      "•\n",
      " \n",
      "Optimized dimensional data models in Redshift, reducing storage costs by 40% while improving query \n",
      "performance\n",
      " \n",
      "•\n",
      " \n",
      "Collaborated with data scientists to build feature stores supporting 15+ ML models in production\n",
      " \n",
      "•\n",
      " \n",
      "Implemented CI/CD practices for data pipelines using Jenkins and version\n",
      "-\n",
      "controlled SQL/Python code \n",
      "in Git\n",
      " \n",
      "Data Engineer\n",
      " \n",
      "| Analytics Pro | Austin, TX | January 2017 \n",
      "–\n",
      " \n",
      "February 2018\n",
      " \n",
      "•\n",
      " \n",
      "Built scalable ETL processes using Python and Spark to integrate data from 20+ external APIs and \n",
      "databases\n",
      " \n",
      "•\n",
      " \n",
      "Developed data validation frameworks ensuring 99.5% data accuracy across financial reporting \n",
      "systems\n",
      " \n",
      "•\n",
      " \n",
      "Created automated monitoring and alerting systems for pipeline failures, reducing MTTR by 50%\n",
      " \n",
      "•\n",
      " \n",
      "Partnered with BI team to design star schema models supporting executive dashboards in Tableau\n",
      " \n",
      "Junior Data Engineer\n",
      " \n",
      "| StartupXYZ | Boston, MA | July 2015 \n",
      "–\n",
      " \n",
      "December 2016\n",
      " \n",
      "•\n",
      " \n",
      "Developed Python scripts to automate data extraction from REST APIs and load into PostgreSQL \n",
      "databases\n",
      " \n",
      "•\n",
      " \n",
      "Assisted in building batch processing pipelines using Apache Spark for customer behavior analytics\n",
      " \n",
      "•\n",
      " \n",
      "Created SQL queries and stored procedures to support reporting requirements for product and \n",
      "marketing teams\n",
      " \n",
      "•\n",
      " \n",
      "Maintained data documentation and lineage using internal wiki and metadata management tools\n",
      "-------THIS IS A CUSTOM END OF PAGE-------\n",
      "EDUCATION\n",
      " \n",
      "Master of Science in Computer Science\n",
      " \n",
      "| Stanford University | 2015\n",
      " \n",
      "Bachelor of Science in Computer Engineering\n",
      " \n",
      "| University of California, Berkeley | 2013\n",
      " \n",
      "CERTIFICATIONS\n",
      " \n",
      "AWS Certified Data Analytics \n",
      "–\n",
      " \n",
      "Specialty | Google Cloud Professional Data Engineer | Databricks Certified \n",
      "Data Engineer\n"
     ]
    }
   ],
   "source": [
    "loader = PyPDFLoader(\n",
    "    \"sample1.pdf\",\n",
    "    mode=\"single\",\n",
    "    pages_delimiter=\"\\n-------THIS IS A CUSTOM END OF PAGE-------\\n\",\n",
    ")\n",
    "pdf_data  = loader.load()\n",
    "print(docs[0].page_content[:5780])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cf558592-b10b-43b3-84ca-bb93fb004c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_extract_resume_pdf = PromptTemplate.from_template(\n",
    " \"\"\"\n",
    " ##Job Description\n",
    " {pdf_data}\n",
    " ### Instruction:\n",
    " The scrapted text is from the pdf of the user. The instruction for you is to extract the skills, certifications and relavent experience and return them in JSON format containing the \n",
    " follwing keys:\n",
    "\n",
    "Skills:\n",
    "Experience (3-4 lines based on the resume):\n",
    "Certification:\n",
    "\n",
    "Only return the valid JSON file.\n",
    "\n",
    "### Valid JSON Format without any preamble\n",
    " \n",
    " \"\"\" \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ddf28472-9f7a-4fb1-9e91-ffed0c68cd00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"Skills\": {\n",
      "    \"Languages\": [\"Python\", \"SQL\", \"Scala\", \"Java\", \"Bash\"],\n",
      "    \"Data Processing\": [\"Apache Spark\", \"Kafka\", \"Airflow\", \"Databricks\", \"dbt\", \"Flink\"],\n",
      "    \"Cloud Platforms\": [\"AWS (Redshift, EMR, Glue, S3, Lambda)\", \"Azure (Synapse, Data Factory)\", \"GCP (BigQuery, Dataflow)\"],\n",
      "    \"Databases\": [\"PostgreSQL\", \"MySQL\", \"MongoDB\", \"Cassandra\", \"Redis\", \"Snowflake\"],\n",
      "    \"Tools & Frameworks\": [\"Docker\", \"Kubernetes\", \"Terraform\", \"Git\", \"Jenkins\", \"Tableau\", \"Looker\"]\n",
      "  },\n",
      "  \"Experience\": [\n",
      "    \"Senior Data Engineer at TechCorp Inc. (June 2020 - Present)\",\n",
      "    \"Architected and deployed real-time streaming data pipelines using Kafka and Spark Streaming, processing 5TB+ daily across 200+ microservices\",\n",
      "    \"Reduced data warehouse query latency by 65% through optimization of Snowflake schemas and implementation of incremental materialized views\",\n",
      "    \"Led migration of legacy ETL processes to modern ELT framework using dbt and Airflow, improving data freshness from 24hrs to 2hrs\"\n",
      "  ],\n",
      "  \"Certification\": [\n",
      "    \"AWS Certified Data Analytics Specialty\",\n",
      "    \"Google Cloud Professional Data Engineer\",\n",
      "    \"Databricks Certified Data Engineer\"\n",
      "  ]\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "chain_extract_pdf_data = prompt_extract_resume_pdf | llm\n",
    "res_pdf = chain_extract_pdf_data.invoke(input = {'pdf_data': pdf_data})\n",
    "print(res_pdf.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e4118df4-e237-4365-ad1e-da4ed4252b5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Skills': {'Languages': ['Python', 'SQL', 'Scala', 'Java', 'Bash'],\n",
       "  'Data Processing': ['Apache Spark',\n",
       "   'Kafka',\n",
       "   'Airflow',\n",
       "   'Databricks',\n",
       "   'dbt',\n",
       "   'Flink'],\n",
       "  'Cloud Platforms': ['AWS (Redshift, EMR, Glue, S3, Lambda)',\n",
       "   'Azure (Synapse, Data Factory)',\n",
       "   'GCP (BigQuery, Dataflow)'],\n",
       "  'Databases': ['PostgreSQL',\n",
       "   'MySQL',\n",
       "   'MongoDB',\n",
       "   'Cassandra',\n",
       "   'Redis',\n",
       "   'Snowflake'],\n",
       "  'Tools & Frameworks': ['Docker',\n",
       "   'Kubernetes',\n",
       "   'Terraform',\n",
       "   'Git',\n",
       "   'Jenkins',\n",
       "   'Tableau',\n",
       "   'Looker']},\n",
       " 'Experience': ['Senior Data Engineer at TechCorp Inc. (June 2020 - Present)',\n",
       "  'Architected and deployed real-time streaming data pipelines using Kafka and Spark Streaming, processing 5TB+ daily across 200+ microservices',\n",
       "  'Reduced data warehouse query latency by 65% through optimization of Snowflake schemas and implementation of incremental materialized views',\n",
       "  'Led migration of legacy ETL processes to modern ELT framework using dbt and Airflow, improving data freshness from 24hrs to 2hrs'],\n",
       " 'Certification': ['AWS Certified Data Analytics Specialty',\n",
       "  'Google Cloud Professional Data Engineer',\n",
       "  'Databricks Certified Data Engineer']}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "json_parser = JsonOutputParser()\n",
    "json_parsed_pdf_data = json_parser.parse(res_pdf.content)\n",
    "json_parsed_pdf_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ca6364-2d6f-4243-af1a-3cc009ccac17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c9373573-7dd1-43aa-96a3-0588ed8a3102",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_comparsion = PromptTemplate.from_template(\n",
    " \"\"\"\n",
    " ##Job Description\n",
    " {json_parsed_job_data}, \n",
    " \n",
    " ###User resume data \n",
    " {json_parsed_pdf_data}\n",
    " ### Instruction:\n",
    "\n",
    " \n",
    " You are a smart job matcher. You already have two jsons. The jsons are returned from job description and the pdf (resume) which the user uploads.\n",
    " Instructions for you\n",
    " 1. Compare both the job skills vs pdf data json and return the matching skills. Use semantic searching matching\n",
    " 2. Return the list of skills that the user lacks based on the job data\n",
    " 3. Give a score out of 10 based on the job suitability\n",
    " 4. Give a one-two liner whether you advise the person to apply for the job or not purely based on the skills provided\n",
    "\n",
    "### Return a response in the following JSON format without preamble:\n",
    "\n",
    "Matched Skills: \n",
    "Unmatched Skills:\n",
    "Job Suitability Score\n",
    "Advice:\n",
    "\n",
    "\n",
    " \"\"\" \n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fc0ebcdc-661f-4884-bc15-d2220b8d79bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"Matched Skills\": [\n",
      "    \"Hadoop HDFS\",\n",
      "    \"Hive\",\n",
      "    \"Pig\",\n",
      "    \"Flume\",\n",
      "    \"Sqoop\",\n",
      "    \"Kafka\",\n",
      "    \"Spark-streaming\",\n",
      "    \"Infrastructure as Code (IaC) practices and frameworks like Terraform\",\n",
      "    \"Java microservices and Spring Boot\",\n",
      "    \"PostgreSQL\"\n",
      "  ],\n",
      "  \"Unmatched Skills\": [\n",
      "    \"Google Cloud Platform Databases (SQL, Spanner)\",\n",
      "    \"HQL\",\n",
      "    \"Cloud Platforms\",\n",
      "    \"Databases\",\n",
      "    \"Tools & Frameworks\"\n",
      "  ],\n",
      "  \"Job Suitability Score\": 8,\n",
      "  \"Advice\": \"Based on the skills provided, it is advised to apply for the job as the candidate has a strong foundation in data engineering and related technologies, but may require additional learning to fill the gaps in Google Cloud Platform and HQL.\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Matched Skills': ['Hadoop HDFS',\n",
       "  'Hive',\n",
       "  'Pig',\n",
       "  'Flume',\n",
       "  'Sqoop',\n",
       "  'Kafka',\n",
       "  'Spark-streaming',\n",
       "  'Infrastructure as Code (IaC) practices and frameworks like Terraform',\n",
       "  'Java microservices and Spring Boot',\n",
       "  'PostgreSQL'],\n",
       " 'Unmatched Skills': ['Google Cloud Platform Databases (SQL, Spanner)',\n",
       "  'HQL',\n",
       "  'Cloud Platforms',\n",
       "  'Databases',\n",
       "  'Tools & Frameworks'],\n",
       " 'Job Suitability Score': 8,\n",
       " 'Advice': 'Based on the skills provided, it is advised to apply for the job as the candidate has a strong foundation in data engineering and related technologies, but may require additional learning to fill the gaps in Google Cloud Platform and HQL.'}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_extract_jop_matching = prompt_comparsion | llm\n",
    "res_matching = chain_extract_jop_matching.invoke(input = {'json_parsed_job_data': json_parsed_job_data, \"json_parsed_pdf_data\":json_parsed_pdf_data})\n",
    "print(res_matching.content)\n",
    "\n",
    "\n",
    "json_parsed_final_output = json_parser.parse(res_matching.content)\n",
    "json_parsed_final_output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab96815-76b8-4799-a3f7-abd9ae7173c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
